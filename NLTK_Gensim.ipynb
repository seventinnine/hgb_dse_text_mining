{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d49afba-1ae8-4b99-adfb-4e3f0789bbac",
   "metadata": {},
   "source": [
    "### Loading our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95502191-b13c-4fea-bbcc-69f9d29e5fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "from typing import List, Set, Tuple\n",
    "\n",
    "# english data\n",
    "classes_en = {1: \"World\", 2: \"Sports\", 3: \"Business\", 4: \"Sci/Tech\"}\n",
    "train_en = pd.read_csv(\"https://raw.githubusercontent.com/michabirklbauer/hgb_dse_text_mining/master/data/AGNews/train.csv\", \n",
    "                       names = [\"Label\", \"Title\", \"Article\"],\n",
    "                       encoding = \"utf-8\")\n",
    "test_en = pd.read_csv(\"https://raw.githubusercontent.com/michabirklbauer/hgb_dse_text_mining/master/data/AGNews/test.csv\", \n",
    "                      names = [\"Label\", \"Title\", \"Article\"],\n",
    "                      encoding = \"utf-8\")\n",
    "\n",
    "# german data\n",
    "train_de = pd.read_csv(\"https://raw.githubusercontent.com/michabirklbauer/hgb_dse_text_mining/master/data/10kGNAD/train.csv\", \n",
    "                       sep = \";\", names = [\"Label\", \"Article\"], \n",
    "                       quotechar = \"\\'\", quoting = csv.QUOTE_MINIMAL, encoding = \"utf-8\")\n",
    "test_de = pd.read_csv(\"https://raw.githubusercontent.com/michabirklbauer/hgb_dse_text_mining/master/data/10kGNAD/test.csv\", \n",
    "                       sep = \";\", names = [\"Label\", \"Article\"], \n",
    "                       quotechar = \"\\'\", quoting = csv.QUOTE_MINIMAL, encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13421461-7335-4c46-9b9f-2d4e134eea2a",
   "metadata": {},
   "source": [
    "By iterating over the dataframe columns we can construct a \"vanilla\" list of documents that we can work on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9e3169c-0449-41c2-9535-68b7a3f5aa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_en = [classes_en[int(row[\"Label\"])] for i, row in train_en.iterrows()]\n",
    "articles_en = [row[\"Article\"] for i, row in train_en.iterrows()]\n",
    "labels_de = [row[\"Label\"] for i, row in train_de.iterrows()]\n",
    "articles_de = [row[\"Article\"] for i, row in train_de.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1a3a938-d719-4300-925b-7734d9480567",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\",\n",
       " 'Reuters - Private investment firm Carlyle Group,\\\\which has a reputation for making well-timed and occasionally\\\\controversial plays in the defense industry, has quietly placed\\\\its bets on another part of the market.',\n",
       " 'Reuters - Soaring crude prices plus worries\\\\about the economy and the outlook for earnings are expected to\\\\hang over the stock market next week during the depth of the\\\\summer doldrums.',\n",
       " 'Reuters - Authorities have halted oil export\\\\flows from the main pipeline in southern Iraq after\\\\intelligence showed a rebel militia could strike\\\\infrastructure, an oil official said on Saturday.',\n",
       " 'AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_en[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9b7ddb-1d99-45fd-b9d5-e4e70db3e35a",
   "metadata": {},
   "source": [
    "# **NLTK**\n",
    "\n",
    "[https://www.nltk.org/](https://www.nltk.org/)\n",
    "\n",
    "NLTK - short for Natural Language Toolkit - is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning and wrappers for industrial-strength NLP libraries.\n",
    "\n",
    "I mostly use NLTK for preprocessing tasks because it is more light-weight and straight forward than spaCy in my opinion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b54b783-e6f0-4c64-890e-d303a5b8817b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords as nltkStopwords\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8a51cd-3006-4373-b0ce-1e4a5db5b3ac",
   "metadata": {},
   "source": [
    "### NLTK tokenizes documents which are any string variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d409dac-d43a-43a4-ada9-554182772014",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "\n",
    "articles_en_tokenized = [nltk.word_tokenize(doc) for doc in articles_en]\n",
    "articles_de_tokenized = [nltk.word_tokenize(doc) for doc in articles_de]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fb87440-321d-458e-a516-4aac1bf15688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Reuters',\n",
       " '-',\n",
       " 'Short-sellers',\n",
       " ',',\n",
       " 'Wall',\n",
       " 'Street',\n",
       " \"'s\",\n",
       " 'dwindling\\\\band',\n",
       " 'of',\n",
       " 'ultra-cynics',\n",
       " ',',\n",
       " 'are',\n",
       " 'seeing',\n",
       " 'green',\n",
       " 'again',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_en_tokenized[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8408f4b3-ed37-4c25-b4ad-3e03e0fbedcb",
   "metadata": {},
   "source": [
    "### Stemming can be done with NLTK's Snowball Stemmer\n",
    "\n",
    "[https://www.nltk.org/api/nltk.stem.snowball.html](https://www.nltk.org/api/nltk.stem.snowball.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "461d8815-e62b-4fb4-b013-d3339df493e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(tokenized_document: str, language: str | None = None) -> List[str]:\n",
    "        stemmer = SnowballStemmer(language, ignore_stopwords = False)\n",
    "        return [stemmer.stem(word) for word in tokenized_document]\n",
    "    \n",
    "articles_en_stemmed = [stem(doc, \"english\") for doc in articles_en_tokenized]\n",
    "articles_de_stemmed = [stem(doc, \"german\") for doc in articles_de_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40b328f7-046b-4916-b83f-1802cd415a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reuter',\n",
       " '-',\n",
       " 'short-sel',\n",
       " ',',\n",
       " 'wall',\n",
       " 'street',\n",
       " \"'s\",\n",
       " 'dwindling\\\\band',\n",
       " 'of',\n",
       " 'ultra-cyn',\n",
       " ',',\n",
       " 'are',\n",
       " 'see',\n",
       " 'green',\n",
       " 'again',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_en_stemmed[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03eeedf6-ac68-4c4a-bf40-858bbaeb98ba",
   "metadata": {},
   "source": [
    "### NLTK also offers built-in stopword sets for different languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03cb5fa9-d4f6-4924-8edc-ef0644013585",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "stopwords_en = set(nltkStopwords.words(\"english\"))\n",
    "stopwords_de = set(nltkStopwords.words(\"german\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf13239-77d4-4fbd-87bb-0bf83e39b2a6",
   "metadata": {},
   "source": [
    "### The english stopwords are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "952154b1-b41f-450a-a956-324666630d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"that'll,which,own,m,does,at,you'd,me,weren,did,won't,mustn't,same,i,because,as,out,hasn,themselves,am,when,wouldn't,its,such,aren't,needn,don,hadn,ours,it's,most,needn't,very,ma,it,weren't,there,had,above,this,through,isn't,from,y,doesn't,have,couldn't,couldn,down,being,shan't,you'll,nor,so,mustn,should,won,after,only,her,with,during,doing,before,both,you,hers,then,theirs,now,mightn't,a,himself,few,your,up,will,but,by,below,again,haven,what,those,them,don't,who,while,into,under,some,he,shouldn't,yourselves,myself,between,too,she,in,him,ain,shouldn,an,were,do,are,just,wasn,against,other,aren,mightn,hasn't,over,why,has,can,be,you're,we,whom,once,that,having,to,than,t,re,was,is,off,how,all,hadn't,our,should've,or,these,further,here,where,ourselves,doesn,ve,and,for,no,haven't,isn,s,didn,shan,his,wasn't,itself,their,until,been,about,yourself,if,she's,you've,on,ll,d,o,my,each,of,the,any,not,they,didn't,herself,wouldn,yours,more\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\",\".join(stopwords_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17cfe54-5d57-4659-b2ca-47fefd178f3d",
   "metadata": {},
   "source": [
    "### And the german ones are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "faa15a4a-513c-419a-a06b-f9ab145bb119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'allem,jeder,machen,hab,sondern,hinter,jene,uns,sind,dort,hin,euren,im,demselben,unsere,während,eurer,am,ihre,ihren,nur,euer,es,könnte,meines,welchem,war,wo,denn,doch,dem,dazu,solchen,haben,meinem,über,keinem,aber,manchem,der,um,ihnen,ob,zum,soll,hatten,manchen,anders,dieselben,jede,dieses,mir,ihres,derselben,anderer,welcher,so,einigen,keine,sein,keines,diese,weg,wollte,ihrem,unseres,dein,dieser,indem,jedem,weil,werden,zur,deine,jenes,würden,desselben,auf,dieselbe,nun,waren,ihm,deiner,als,sollte,sehr,hat,dass,solches,wird,dir,aus,eures,jenem,denselben,diesen,will,damit,ein,einem,alles,noch,daß,muss,etwas,deinen,anderm,auch,würde,jener,mein,bin,des,weiter,dasselbe,allen,solcher,dessen,wenn,wieder,seiner,anderr,einer,euch,in,nach,oder,da,ihrer,an,deinem,dich,unter,unser,werde,ander,ich,vor,anderem,nichts,diesem,einiger,aller,bis,meine,selbst,andern,dann,für,einiges,welches,seines,einigem,durch,hatte,nicht,meiner,jenen,seinen,jetzt,mit,keiner,einig,was,eure,er,unseren,seine,sich,von,wie,ohne,wollen,die,eines,andere,derer,gegen,welchen,sie,unserem,und,einmal,hier,eine,einen,können,zwischen,anderen,man,manche,bist,vom,gewesen,dies,kein,kann,zu,einige,du,solche,deines,ihn,keinen,seinem,anderes,eurem,ist,bei,sonst,wirst,musste,jedes,zwar,solchem,viel,meinen,warst,derselbe,ihr,manches,wir,mich,ins,das,jeden,den,also,habe,alle,welche,mancher'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\",\".join(stopwords_de)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bb452b-ba84-4c1c-8ba8-5066a2910167",
   "metadata": {},
   "source": [
    "### Removing stopwords from our stemmed documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f4b0f60-11c7-4d59-b080-07e186f3238e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(stemmed_document: str, stopwords: Set) -> List[str]:\n",
    "        def is_stopword(word):\n",
    "            return not word in stopwords\n",
    "        return list(filter(is_stopword, stemmed_document))\n",
    "    \n",
    "articles_en_final = [remove_stopwords(doc, stopwords_en) for doc in articles_en_stemmed]\n",
    "articles_de_final = [remove_stopwords(doc, stopwords_de) for doc in articles_de_stemmed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea6e1ce3-03f9-4c07-b5f0-8687c429ecca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reuter',\n",
       " '-',\n",
       " 'short-sel',\n",
       " ',',\n",
       " 'wall',\n",
       " 'street',\n",
       " \"'s\",\n",
       " 'dwindling\\\\band',\n",
       " 'ultra-cyn',\n",
       " ',',\n",
       " 'see',\n",
       " 'green',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_en_final[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400603ab-fd34-442f-83cb-bc6fa423515e",
   "metadata": {},
   "source": [
    "# **Gensim**\n",
    "\n",
    "[https://radimrehurek.com/gensim/](https://radimrehurek.com/gensim/)\n",
    "\n",
    "Gensim titles itself as \"Topic Modelling for Humans\" and is the third and final NLP library that we will have a look at. I have mainly used Gensim to build TF-IDF models and run text queries on datasets. We are going to use our NLTK preprocessed documents as input to build a dictionary, corpus and index with Gensim and calculate the TF-IDF matrix to run text queries on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4da13794-2d08-46dc-920c-8acbbffff714",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim import similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6b53d3-6dcd-4b60-9639-2a112e161d7a",
   "metadata": {},
   "source": [
    "### Building the TF-IDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee423808-4843-41ef-92b5-7e930498e6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 500 # adjust if model too big\n",
    "corpus_dictionary_en = corpora.Dictionary(articles_en_final[:size])\n",
    "corpus_en = [corpus_dictionary_en.doc2bow(document) for document in articles_en_final[:size]]\n",
    "model_en = models.TfidfModel(corpus_en)\n",
    "index_en = similarities.MatrixSimilarity(model_en[corpus_en])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3234d6-ee85-4fd2-b516-5db5a9f49bb6",
   "metadata": {},
   "source": [
    "To calculate the similarity of our input the query has to be preprocessed the same way our data was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1da0c310-f226-47f7-bb96-536865b5d4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_en(query_string: str) -> List[Tuple[int, float]]:\n",
    "    q = corpus_dictionary_en.doc2bow(remove_stopwords(stem(nltk.word_tokenize(query_string), language = \"english\"), stopwords_en))\n",
    "    q_model = model_en[q]\n",
    "    result = index_en[q_model]\n",
    "    result = sorted(enumerate(result), key = lambda item: -item[1])\n",
    "    for i, j in enumerate(result):\n",
    "        if i > 2:\n",
    "            break\n",
    "        print(j, articles_en[:size][j[0]])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6c3769-8a3b-4ab5-b55a-94fdb159863c",
   "metadata": {},
   "source": [
    "### Gensim returns the resulting document and its similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ac1848f-1f64-4c14-81e9-a9673b508c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(237, 0.3952839) Scientists in the United States find a way to turn lazy monkeys into workaholics using gene therapy.\n",
      "(450, 0.21606433) AFP - National Basketball Association players trying to win a fourth consecutive Olympic gold medal for the United States have gotten the wake-up call that the \"Dream Team\" days are done even if supporters have not.\n",
      "(462, 0.20889904)  ATHENS (Reuters) - The United States beat Canada in a world  best time to qualify for the final of the men's Olympic eights  race Sunday, as the two crews renewed their fierce rivalry in  front of a raucous crowd at Schinias.\n"
     ]
    }
   ],
   "source": [
    "query_en(\"Scientists United States\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
